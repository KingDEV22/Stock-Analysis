{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t4O5VxjFDkO",
        "outputId": "ccdc3d09-3971-43d8-8ca3-c5bb72716465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26429 entries, 0 to 26428\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       26429 non-null  object        \n",
            " 1   Date       26429 non-null  datetime64[ns]\n",
            " 2   Sentiment  26429 non-null  int64         \n",
            "dtypes: datetime64[ns](1), int64(1), object(1)\n",
            "memory usage: 619.6+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1435 entries, 6023 to 7492\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       1435 non-null   object        \n",
            " 1   Date       1435 non-null   datetime64[ns]\n",
            " 2   Sentiment  0 non-null      float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(1)\n",
            "memory usage: 44.8+ KB\n",
            "None\n",
            " 1    16121\n",
            " 0     6119\n",
            "-1     4189\n",
            "Name: Sentiment, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import parfit.parfit as pf\n",
        "\n",
        "##Spacy\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## preprocessing tools\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split,ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,roc_auc_score, f1_score,make_scorer\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "## algorithm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "## tensorflow\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.datasets import imdb\n",
        "# from keras.preprocessing.text import one_hot, Tokenizer\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "# from tensorflow.keras.layers import LSTM\n",
        "# from tensorflow.keras.layers import Embedding\n",
        "# from tensorflow.keras.preprocessing import sequence\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow import keras\n",
        "# from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
        "# from keras.layers import Conv1D\n",
        "\n",
        "#autenticating to google\n",
        "# auth.authenticate_user()\n",
        "# creds, _ = default()\n",
        "# gc = gspread.authorize(creds)\n",
        "\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe('spacytextblob')\n",
        "nlp.pipe_names\n",
        "\n",
        "\n",
        "df = pd.read_csv('stock_data.csv')\n",
        "df = df.iloc[:,1:4]\n",
        "df['Text'] = df['Text'].astype(str)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Sentiment'] = df['Sentiment'].astype(np.int64)\n",
        "print(df.info())\n",
        "\n",
        "test_data = pd.read_csv('2020.csv')\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data= test_data.iloc[:,1:4]\n",
        "test_data.rename(columns = {'Headline':'Text','Target':'Sentiment'}, inplace = True)\n",
        "test_data['Text'] = test_data['Text'].astype(str)\n",
        "test_data =test_data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "test_data = test_data[['Text','Date']].merge(df, on=['Text','Date'], how='left')\n",
        "test_data = test_data[test_data['Text'].str.contains(\"\\?\")==False]\n",
        "test_data = test_data.loc[test_data['Sentiment'].isnull() == True][:5000]\n",
        "print(test_data.info())\n",
        "print(df['Sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_column', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_seq_items', None)\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "pd.set_option('expand_frame_repr', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tok_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sebi to control unsolicited fin market advise from social media influencers</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "      <td>sebi control unsolicited fin market advise social media influencers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sebi pitches change in rules for REITs, InvITs; sponsors to own some units</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "      <td>sebi pitches change rules reits invits sponsors units</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sri Lanka's body approves renewable energy projects of Adani group</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "      <td>sri lanka body approves renewable energy projects adani group</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sebi likely to scrap small town-linked incentive for mutual funds</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "      <td>sebi likely scrap small town linked incentive mutual funds</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indices post biggest weekly decline since June; Sensex falls 2.5%</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>-1</td>\n",
              "      <td>indices post biggest weekly decline june sensex falls</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                          Text  \\\n",
              "0  Sebi to control unsolicited fin market advise from social media influencers   \n",
              "1   Sebi pitches change in rules for REITs, InvITs; sponsors to own some units   \n",
              "2           Sri Lanka's body approves renewable energy projects of Adani group   \n",
              "3            Sebi likely to scrap small town-linked incentive for mutual funds   \n",
              "4            Indices post biggest weekly decline since June; Sensex falls 2.5%   \n",
              "\n",
              "        Date  Sentiment  \\\n",
              "0 2023-02-24          1   \n",
              "1 2023-02-24          1   \n",
              "2 2023-02-24          1   \n",
              "3 2023-02-24          1   \n",
              "4 2023-02-24         -1   \n",
              "\n",
              "                                                              Tok_text  \n",
              "0  sebi control unsolicited fin market advise social media influencers  \n",
              "1                sebi pitches change rules reits invits sponsors units  \n",
              "2        sri lanka body approves renewable energy projects adani group  \n",
              "3           sebi likely scrap small town linked incentive mutual funds  \n",
              "4               indices post biggest weekly decline june sensex falls   "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "RangeIndex: 26429 entries, 0 to 26428\n",
            "Series name: Text\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "26429 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 206.6+ KB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kings\\AppData\\Local\\Temp\\ipykernel_12988\\2740825243.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  data1 = df['Text'].str.replace('[^\\w\\s]', ' ')\n"
          ]
        }
      ],
      "source": [
        "data1 = df['Text'].str.replace('[^\\w\\s]', ' ')\n",
        "data1 = \" \"+data1+\" \"\n",
        "data1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2113 entries, 0 to 2112\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Symbol        2112 non-null   object\n",
            " 1   Company Name  2113 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 33.1+ KB\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('company.csv')\n",
        "data['Company Name'] = data['Company Name'].apply(str).apply(str.lower)\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Symbol</th>\n",
              "      <th>Company Name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RELIANCE</td>\n",
              "      <td>reliance industries limited</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TCS</td>\n",
              "      <td>tata consultancy services limited</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HDFCBANK</td>\n",
              "      <td>hdfc bank limited</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INFY</td>\n",
              "      <td>infosys limited</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ICICIBANK</td>\n",
              "      <td>icici bank limited</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Symbol                       Company Name\n",
              "0   RELIANCE        reliance industries limited\n",
              "1        TCS  tata consultancy services limited\n",
              "2   HDFCBANK                  hdfc bank limited\n",
              "3       INFY                    infosys limited\n",
              "4  ICICIBANK                 icici bank limited"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_news(news_data, company_filter):\n",
        "    news_data['Text'] = news_data['Text'].apply(str.lower)\n",
        "    company_filter['Symbol'] = company_filter['Symbol'].apply(str).apply(str.lower)\n",
        "    company_filter['Company Name'] = company_filter['Company Name'].apply(str).apply(str.lower)\n",
        "    final_data = pd.DataFrame()\n",
        "    news_data['Text'] = news_data['Text'].str.replace('[^\\w\\s]', ' ')\n",
        "    news_data['Text'] = \" \"+news_data['Text']+\" \"\n",
        "\n",
        "    # iterate all company names to find out the match\n",
        "    for i in range(len(company_filter)-1):\n",
        "        try:\n",
        "            keyword = \" \"+company_filter[\"Symbol\"][i]+\" \"\n",
        "            CompanyName = \" \"+company_filter[\"Company Name\"][i]+\" \"\n",
        "            l=[\" global \",\" wealth \",\" take \",\" focus \",\" worth \"]\n",
        "            if(keyword == 'ttl'):\n",
        "                continue\n",
        "            \n",
        "            if(keyword in l):\n",
        "                print(keyword)\n",
        "                contain_values = news_data[(news_data['Text'].str.contains(keyword))&(news_data['Text'].str.contains(CompanyName))]\n",
        "                contain_values['Symbol'] = company_filter[\"Symbol\"][i]\n",
        "                contain_values['Company Name'] = company_filter[\"Company Name\"][i]\n",
        "                final_data = final_data.append(contain_values, ignore_index=True)\n",
        "\n",
        "            else:\n",
        "                contain_values = news_data[(news_data['Text'].str.contains(keyword))|(news_data['Text'].str.contains(CompanyName))]\n",
        "                contain_values['Symbol'] = company_filter[\"Symbol\"][i]\n",
        "                contain_values['Company Name'] = company_filter[\"Company Name\"][i]\n",
        "                final_data = final_data.append(contain_values, ignore_index=True)\n",
        "\n",
        "        except:\n",
        "            print(\"Company name is small at index \", l)\n",
        "\n",
        "    return final_data\n",
        "data1['Headline']=data1['Text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sebi to control unsolicited fin market advise ...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sebi pitches change in rules for REITs, InvITs...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sri Lanka\\s body approves renewable energy pro...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sebi likely to scrap small town-linked incenti...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indices post biggest weekly decline since June...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text       Date  Sentiment\n",
              "0  Sebi to control unsolicited fin market advise ... 2023-02-24          1\n",
              "1  Sebi pitches change in rules for REITs, InvITs... 2023-02-24          1\n",
              "2  Sri Lanka\\s body approves renewable energy pro... 2023-02-24          1\n",
              "3  Sebi likely to scrap small town-linked incenti... 2023-02-24          1\n",
              "4  Indices post biggest weekly decline since June... 2023-02-24         -1"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qaeB_4jkPLnv",
        "outputId": "48e3c9f6-4c05-4071-fe1d-123981f4b105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(21143, 9479) (5286, 9479)\n",
            "(1435, 9479)\n"
          ]
        }
      ],
      "source": [
        "punct = string.punctuation\n",
        "stopwords = list(STOP_WORDS)\n",
        "def text_data_cleaning(sentence):\n",
        "    sent = preprocess_text(sentence)\n",
        "    doc = nlp(sent)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.lemma_ != \"-PRON-\":\n",
        "            temp = token.lemma_.lower().strip()\n",
        "        else:\n",
        "            temp = token.lower_\n",
        "        tokens.append(temp)\n",
        "    \n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stopwords and token not in punct:\n",
        "            cleaned_tokens.append(token)\n",
        "    return append_message(cleaned_tokens)\n",
        "def preprocess_text(sen):\n",
        "    '''Cleans text data up, leaving only 2 or more char long non-stepwords composed of A-Z & a-z only\n",
        "    in lowercase'''\n",
        "    sentence = sen.lower()\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence) \n",
        "    # Remove multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)  \n",
        "    # Remove Stopwords\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
        "    sentence = pattern.sub('', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def append_message(text):\n",
        "  str = \" \"\n",
        "  return (str.join(text))\n",
        "\n",
        "\n",
        "df['Tok_text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "tf_idf_vect = TfidfVectorizer()\n",
        "X = df['Tok_text']\n",
        "y = df['Sentiment']\n",
        "# X = tf_idf_vect.fit_transform(X)\n",
        "\n",
        "##splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "X_train = tf_idf_vect.fit_transform(X_train)\n",
        "X_test = tf_idf_vect.transform(X_test)\n",
        "print(X_train.shape, X_test.shape)\n",
        "## test data preprocessing\n",
        "test_data['Tok_text'] = test_data['Text'].apply(preprocess_text)\n",
        "test = test_data['Tok_text']\n",
        "x_test = tf_idf_vect.transform(test_data['Tok_text'])\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_62toT-1iPB6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For SDG classification\n",
            "Best Score: 0.913399281842967\n",
            "{'alpha': 0.0001, 'loss': 'modified_huber', 'n_jobs': -1, 'penalty': 'l2'}\n",
            "SGDClassifier(loss='modified_huber', n_jobs=-1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.90      0.84      0.87       812\n",
            "           0       0.89      0.87      0.88      1191\n",
            "           1       0.94      0.96      0.95      3283\n",
            "\n",
            "    accuracy                           0.92      5286\n",
            "   macro avg       0.91      0.89      0.90      5286\n",
            "weighted avg       0.92      0.92      0.92      5286\n",
            "\n",
            "For Logistic Regression classification\n",
            "Best Score: 0.9167573628321634\n",
            "{'C': 10, 'multi_class': 'multinomial', 'n_jobs': -1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "LogisticRegression(C=10, multi_class='multinomial', n_jobs=-1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.91      0.87      0.89       812\n",
            "           0       0.90      0.89      0.89      1191\n",
            "           1       0.95      0.96      0.96      3283\n",
            "\n",
            "    accuracy                           0.93      5286\n",
            "   macro avg       0.92      0.91      0.91      5286\n",
            "weighted avg       0.93      0.93      0.93      5286\n",
            "\n"
          ]
        }
      ],
      "source": [
        "score = [0]\n",
        "\n",
        "classifier = SGDClassifier()\n",
        "param_grid = {\n",
        "        'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2], # learning rate # number of epochs\n",
        "        'loss': ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber'], # logistic regression,\n",
        "        'penalty': ['l2'],\n",
        "        'n_jobs': [-1]\n",
        "    }\n",
        "\n",
        "grid1 = GridSearchCV(classifier, param_grid, refit = True, verbose = 0)\n",
        "    # fitting the model for grid search\n",
        "grid1.fit(X_train, y_train)\n",
        "print(\"For SDG classification\")\n",
        "score.append(grid1.best_score_)\n",
        "print('Best Score: %s' % grid1.best_score_)\n",
        "    # print best parameter after tuning\n",
        "print(grid1.best_params_)\n",
        "    # print how our model looks after hyper-parameter tuning\n",
        "print(grid1.best_estimator_)\n",
        "\n",
        "y_pred = grid1.predict(X_test)\n",
        "# print(\"For SDG classification\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "param_grid = {\n",
        "        'C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 1e0],\n",
        "        'penalty': ['l2'],\n",
        "        'n_jobs': [-1],\n",
        "        'multi_class': ['multinomial'],\n",
        "        'solver': ['lbfgs']\n",
        "    }\n",
        "grid2 = GridSearchCV(classifier, param_grid, refit = True, verbose = 0)\n",
        "    # fitting the model for grid search\n",
        "grid2.fit(X_train, y_train)\n",
        "score.append(grid2.best_score_)\n",
        "print(\"For Logistic Regression classification\")\n",
        "print('Best Score: %s' % grid2.best_score_)\n",
        "    # print best parameter after tuning\n",
        "print(grid2.best_params_)\n",
        "    # print how our model looks after hyper-parameter tuning\n",
        "print(grid2.best_estimator_)\n",
        "\n",
        "y_pred = grid2.predict(X_test)\n",
        "# print(\"For Logistic Regression classification\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "num = score.index(max(score))\n",
        "if num == 1:\n",
        "    y_pred = grid1.predict(x_test)\n",
        "    check = pd.DataFrame({'Text':test_data['Text'],'Date':test_data['Date'],'Sentiment':y_pred})\n",
        "    check.to_csv('check.csv')\n",
        "elif num == 2:\n",
        "    y_pred = grid2.predict(x_test)\n",
        "    check = pd.DataFrame({'Text':test_data['Text'],'Date':test_data['Date'],'Sentiment':y_pred})\n",
        "    check.to_csv('check.csv')\n",
        "#     # y_pred = grid4.predict(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Text       5000 non-null   object\n",
            " 1   Date       5000 non-null   object\n",
            " 2   Sentiment  5000 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 117.3+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21429 entries, 0 to 21428\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       21429 non-null  object        \n",
            " 1   Date       21429 non-null  datetime64[ns]\n",
            " 2   Sentiment  21429 non-null  int64         \n",
            " 3   Tok_text   21429 non-null  object        \n",
            "dtypes: datetime64[ns](1), int64(1), object(2)\n",
            "memory usage: 669.8+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26429 entries, 0 to 26428\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Text       26429 non-null  object\n",
            " 1   Date       26429 non-null  object\n",
            " 2   Sentiment  26429 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 619.6+ KB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kings\\AppData\\Local\\Temp\\ipykernel_12988\\1174786404.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fg = df.append(df1,ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "df1 = pd.read_csv('check.csv')\n",
        "df1.drop(columns=['Unnamed: 0'], axis=1,  inplace=True)\n",
        "df1.rename(columns={'0' : 'Sentiment'},inplace=True)\n",
        "index_name = df1[df1['Sentiment'].isna()==True].index\n",
        "df1.drop(index_name,inplace=True)\n",
        "print(df1.info())\n",
        "print(df.info())\n",
        "df.drop(columns=['Tok_text'], axis=1,  inplace=True)\n",
        "fg = df.append(df1,ignore_index=True)\n",
        "fg.info()\n",
        "fg.to_csv('stock_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       100 non-null    object        \n",
            " 1   Date       100 non-null    datetime64[ns]\n",
            " 2   Sentiment  0 non-null      float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(1)\n",
            "memory usage: 3.1+ KB\n"
          ]
        }
      ],
      "source": [
        "# test_data = pd.read_csv('deb1.csv')\n",
        "# test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "# test_data= test_data.iloc[:,1:4]\n",
        "# test_data.rename(columns = {'Headline':'Text','Target':'Sentiment'}, inplace = True)\n",
        "# test_data['Text'] = test_data['Text'].astype(str)\n",
        "# test_data =test_data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "# test_data = test_data[['Text','Date']].merge(df, on=['Text','Date'], how='left')\n",
        "# test_data = test_data[test_data['Text'].str.contains(\"\\?\")==False]\n",
        "# test_data = test_data.loc[test_data['Sentiment'].isnull() == True][:100]\n",
        "# test_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4000 entries, 0 to 3999\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       4000 non-null   object        \n",
            " 1   Date       4000 non-null   datetime64[ns]\n",
            " 2   Sentiment  0 non-null      float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(1)\n",
            "memory usage: 125.0+ KB\n"
          ]
        }
      ],
      "source": [
        "# test_data = pd.read_csv('soh1.csv')\n",
        "# test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "# test_data= test_data.iloc[:,1:4]\n",
        "# test_data.rename(columns = {'Headline':'Text','Target':'Sentiment'}, inplace = True)\n",
        "# test_data['Text'] = test_data['Text'].astype(str)\n",
        "# test_data =test_data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "# test_data = test_data[['Text','Date']].merge(df, on=['Text','Date'], how='left')\n",
        "# test_data = test_data[test_data['Text'].str.contains(\"\\?\")==False]\n",
        "# test_data = test_data.loc[test_data['Sentiment'].isnull() == True]\n",
        "# test_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       100 non-null    object        \n",
            " 1   Date       100 non-null    datetime64[ns]\n",
            " 2   Sentiment  0 non-null      float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(1)\n",
            "memory usage: 3.1+ KB\n"
          ]
        }
      ],
      "source": [
        "# test_data = pd.read_csv('say1.csv')\n",
        "# test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "# test_data= test_data.iloc[:,1:4]\n",
        "# test_data.rename(columns = {'Headline':'Text','Target':'Sentiment'}, inplace = True)\n",
        "# test_data['Text'] = test_data['Text'].astype(str)\n",
        "# test_data =test_data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "# test_data = test_data[['Text','Date']].merge(df, on=['Text','Date'], how='left')\n",
        "# test_data = test_data[test_data['Text'].str.contains(\"\\?\")==False]\n",
        "# test_data = test_data.loc[test_data['Sentiment'].isnull() == True][:100]\n",
        "# test_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = grid.predict(x_test)\n",
        "check = pd.DataFrame({'Text':test_data['Text'],'Date':test_data['Date'],'Sentiment':y_pred})\n",
        "check.to_csv('check.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7aRnGH_3EFD"
      },
      "source": [
        "Using CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Text       100 non-null    object\n",
            " 1   Date       100 non-null    object\n",
            " 2   Sentiment  100 non-null    int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.5+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2187 entries, 0 to 2186\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       2187 non-null   object        \n",
            " 1   Date       2187 non-null   datetime64[ns]\n",
            " 2   Sentiment  2187 non-null   int64         \n",
            "dtypes: datetime64[ns](1), int64(1), object(1)\n",
            "memory usage: 51.4+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2387 entries, 0 to 2386\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Text       2387 non-null   object\n",
            " 1   Date       2387 non-null   object\n",
            " 2   Sentiment  2387 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 56.1+ KB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kings\\AppData\\Local\\Temp\\ipykernel_16836\\1129578224.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fg = df.append(df1,ignore_index=True)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df2 = test_data[['Text','Date']].merge(df1, on=['Text'], how='left')\n",
        "# index_name = df2[df2['Sentiment'].isna()==True].index\n",
        "# df2.drop(index_name,inplace=True)\n",
        "# df2.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOzc0IBWtr-a"
      },
      "outputs": [],
      "source": [
        "# pos_msg = df[df['Sentiment'] == 1]\n",
        "# zeo_msg = df[df['Sentiment']==0]\n",
        "# nrg_msg = df[df['Sentiment']==-1]\n",
        "# pos_msg_text = \" \".join(pos_msg.Tok_text.to_numpy().tolist())\n",
        "# zeo_msg_text = \" \".join(zeo_msg.Tok_text.to_numpy().tolist())\n",
        "# nrg_msg_text = \" \".join(nrg_msg.Tok_text.to_numpy().tolist())\n",
        "\n",
        "# pos_msg_cloud = WordCloud(width =520, height =260, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(pos_msg_text)\n",
        "# plt.figure(figsize=(16,10))\n",
        "# plt.imshow(pos_msg_cloud, interpolation='bilinear')\n",
        "# plt.axis('off') # turn off axis\n",
        "# plt.show()\n",
        "# zeo_msg_cloud = WordCloud(width =520, height =260, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(zeo_msg_text)\n",
        "# plt.figure(figsize=(16,10))\n",
        "# plt.imshow(zeo_msg_cloud, interpolation='bilinear')\n",
        "# plt.axis('off') # turn off axis\n",
        "# plt.show()\n",
        "# nrg_msg_cloud = WordCloud(width =520, height =260, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(nrg_msg_text)\n",
        "# plt.figure(figsize=(16,10))\n",
        "# plt.imshow(nrg_msg_cloud, interpolation='bilinear')\n",
        "# plt.axis('off') # turn off axis\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NeynTHpSsYR"
      },
      "outputs": [],
      "source": [
        "# fix random seed for reproducibility\n",
        "jj\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "\n",
        "top_words = 5000\n",
        "#(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "X = []\n",
        "y = df['Sentiment']\n",
        "sentences = list(df['Text'])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(X_train)\n",
        "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
        "X_test = word_tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "vocab_length = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/drive/My Drive/Colab Notebooks/a2_glove.6B.100d.txt', encoding=\"utf8\")#/content/drive/MyDrive/Colab Notebooks/a2_glove.6B.100d.txt\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "\n",
        "embedding_matrix = zeros((vocab_length, 100))\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6f0PEILvYpJ"
      },
      "outputs": [],
      "source": [
        "# Neural Network architecture\n",
        "\n",
        "cnn_model = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "cnn_model.add(embedding_layer)\n",
        "\n",
        "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
        "cnn_model.add(GlobalMaxPooling1D())\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUBLeVUCva_n"
      },
      "outputs": [],
      "source": [
        "# Model compiling\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(cnn_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6Y6Ki_MvcwT"
      },
      "outputs": [],
      "source": [
        "cnn_model_history = cnn_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1Z0mZMjvfPr"
      },
      "outputs": [],
      "source": [
        "score = cnn_model.evaluate(X_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmYvcBD8vvpK"
      },
      "outputs": [],
      "source": [
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S9gqfPA45oF"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Dense(\n",
        "      hp.Choice('units', [8, 16, 32]),\n",
        "      activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='relu'))\n",
        "  model.compile(loss='mse')\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUO6s3B05VVz"
      },
      "outputs": [],
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rKLMDoE5X4E"
      },
      "outputs": [],
      "source": [
        "tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnCO2JIM9CBs"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdmnFwdq_fhK"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9flQzdny9Jk4"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.0002, \n",
        "    epsilon=1e-08, \n",
        "    clipnorm=1.0),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FuEruMjAFpl"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, epochs=2, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG25WZnlArBH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wRKqtbWUldM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AZBHiXEUlZ9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df078hbRUlXg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMuwdS-TUlVD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZfA7q06UlSO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCuTRWMMUlPa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9NjhO3IUlKa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1o2M4iMUmP5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqPlYx-kWP-h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
