{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t4O5VxjFDkO",
        "outputId": "ccdc3d09-3971-43d8-8ca3-c5bb72716465"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tok2vec',\n",
              " 'tagger',\n",
              " 'parser',\n",
              " 'attribute_ruler',\n",
              " 'lemmatizer',\n",
              " 'ner',\n",
              " 'spacytextblob']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import parfit.parfit as pf\n",
        "\n",
        "##Spacy\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## preprocessing tools\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold,GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split,ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,roc_auc_score, f1_score,make_scorer\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "## algorithm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "## tensorflow\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.datasets import imdb\n",
        "# from keras.preprocessing.text import one_hot, Tokenizer\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "# from tensorflow.keras.layers import LSTM\n",
        "# from tensorflow.keras.layers import Embedding\n",
        "# from tensorflow.keras.preprocessing import sequence\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow import keras\n",
        "# from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
        "# from keras.layers import Conv1D\n",
        "\n",
        "#autenticating to google\n",
        "# auth.authenticate_user()\n",
        "# creds, _ = default()\n",
        "# gc = gspread.authorize(creds)\n",
        "\n",
        "\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe('spacytextblob')\n",
        "nlp.pipe_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ige6ZjSpb_Gd",
        "outputId": "01963402-b09f-4de6-f5b3-838965520eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1887 entries, 0 to 1886\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       1887 non-null   object        \n",
            " 1   Date       1887 non-null   datetime64[ns]\n",
            " 2   Sentiment  1887 non-null   int64         \n",
            "dtypes: datetime64[ns](1), int64(1), object(1)\n",
            "memory usage: 44.4+ KB\n",
            "None\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sebi to control unsolicited fin market advise ...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sebi pitches change in rules for REITs, InvITs...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sri Lanka's body approves renewable energy pro...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sebi likely to scrap small town-linked incenti...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indices post biggest weekly decline since June...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text       Date  Sentiment\n",
              "0  Sebi to control unsolicited fin market advise ... 2023-02-24          1\n",
              "1  Sebi pitches change in rules for REITs, InvITs... 2023-02-24          1\n",
              "2  Sri Lanka's body approves renewable energy pro... 2023-02-24          1\n",
              "3  Sebi likely to scrap small town-linked incenti... 2023-02-24          1\n",
              "4  Indices post biggest weekly decline since June... 2023-02-24         -1"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('stock_data.csv')\n",
        "df = df.iloc[:,1:4]\n",
        "df['Text'] = df['Text'].astype(str)\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Sentiment'] = df['Sentiment'].astype(np.int64)\n",
        "print(df.info())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\kings\\OneDrive\\Documents\\Dev\\Stock-Analysis\\envr\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname IST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
            "c:\\Users\\kings\\OneDrive\\Documents\\Dev\\Stock-Analysis\\envr\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname IS identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
            "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1838</th>\n",
              "      <td>Nykaa and Policy Bazaar slide as one-year lock...</td>\n",
              "      <td>2022-10-25</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1839</th>\n",
              "      <td>Laurus Labs hits 4-month low; stock slips 14% ...</td>\n",
              "      <td>2022-10-25</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1841</th>\n",
              "      <td>This film production &amp; distribution company st...</td>\n",
              "      <td>2022-10-25</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1842</th>\n",
              "      <td>RIL: Bulls set eyes on Rs 3,500-mark as stock ...</td>\n",
              "      <td>2022-10-25</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>Nykaa hits record low; slips below issue price...</td>\n",
              "      <td>2022-10-25</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text       Date  Sentiment\n",
              "1838  Nykaa and Policy Bazaar slide as one-year lock... 2022-10-25        NaN\n",
              "1839  Laurus Labs hits 4-month low; stock slips 14% ... 2022-10-25        NaN\n",
              "1841  This film production & distribution company st... 2022-10-25        NaN\n",
              "1842  RIL: Bulls set eyes on Rs 3,500-mark as stock ... 2022-10-25        NaN\n",
              "1843  Nykaa hits record low; slips below issue price... 2022-10-25        NaN"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data = pd.read_csv('s1.csv')\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data= test_data.iloc[:,1:4]\n",
        "test_data.rename(columns = {'Headline':'Text','Target':'Sentiment'}, inplace = True)\n",
        "test_data['Text'] = test_data['Text'].astype(str)\n",
        "test_data =test_data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "test_data = test_data[['Text','Date']].merge(df, on=['Text','Date'], how='left')\n",
        "test_data = test_data[test_data['Text'].str.contains(\"\\?\")==False]\n",
        "test_data = test_data.loc[test_data['Sentiment'].isnull() == True][:100]\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       100 non-null    object        \n",
            " 1   Date       100 non-null    datetime64[ns]\n",
            " 2   Sentiment  0 non-null      float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(1)\n",
            "memory usage: 3.1+ KB\n"
          ]
        }
      ],
      "source": [
        "test_data = pd.read_csv('deb1.csv')\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data= test_data.iloc[:,1:4]\n",
        "test_data.rename(columns = {'Headline':'Text','Target':'Sentiment'}, inplace = True)\n",
        "test_data['Text'] = test_data['Text'].astype(str)\n",
        "test_data =test_data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "test_data = test_data[['Text','Date']].merge(df, on=['Text','Date'], how='left')\n",
        "test_data = test_data[test_data['Text'].str.contains(\"\\?\")==False]\n",
        "test_data = test_data.loc[test_data['Sentiment'].isnull() == True][:100]\n",
        "test_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       100 non-null    object        \n",
            " 1   Date       100 non-null    datetime64[ns]\n",
            " 2   Sentiment  0 non-null      float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(1)\n",
            "memory usage: 3.1+ KB\n"
          ]
        }
      ],
      "source": [
        "test_data = pd.read_csv('soh1.csv')\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data= test_data.iloc[:,1:4]\n",
        "test_data.rename(columns = {'Headline':'Text','Target':'Sentiment'}, inplace = True)\n",
        "test_data['Text'] = test_data['Text'].astype(str)\n",
        "test_data =test_data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "test_data = test_data[['Text','Date']].merge(df, on=['Text','Date'], how='left')\n",
        "test_data = test_data[test_data['Text'].str.contains(\"\\?\")==False]\n",
        "test_data = test_data.loc[test_data['Sentiment'].isnull() == True][:100]\n",
        "test_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   Text       100 non-null    object        \n",
            " 1   Date       100 non-null    datetime64[ns]\n",
            " 2   Sentiment  0 non-null      float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(1)\n",
            "memory usage: 3.1+ KB\n"
          ]
        }
      ],
      "source": [
        "test_data = pd.read_csv('say1.csv')\n",
        "test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "test_data= test_data.iloc[:,1:4]\n",
        "test_data.rename(columns = {'Headline':'Text','Target':'Sentiment'}, inplace = True)\n",
        "test_data['Text'] = test_data['Text'].astype(str)\n",
        "test_data =test_data.replace(r'^\\s*$', np.nan, regex=True)\n",
        "test_data = test_data[['Text','Date']].merge(df, on=['Text','Date'], how='left')\n",
        "test_data = test_data[test_data['Text'].str.contains(\"\\?\")==False]\n",
        "test_data = test_data.loc[test_data['Sentiment'].isnull() == True][:100]\n",
        "test_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiHwzq4e7Q-e",
        "outputId": "01a8d71c-e5fa-4adb-d333-a89f6902e480"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              " 1    954\n",
              "-1    520\n",
              " 0    413\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qaeB_4jkPLnv",
        "outputId": "48e3c9f6-4c05-4071-fe1d-123981f4b105"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Tok_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sebi to control unsolicited fin market advise ...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "      <td>sebi control unsolicited fin market advise soc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sebi pitches change in rules for REITs, InvITs...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "      <td>sebi pitches change rules reits invits sponsor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sri Lanka's body approves renewable energy pro...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "      <td>sri lanka body approves renewable energy proje...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sebi likely to scrap small town-linked incenti...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>1</td>\n",
              "      <td>sebi likely scrap small town linked incentive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Indices post biggest weekly decline since June...</td>\n",
              "      <td>2023-02-24</td>\n",
              "      <td>-1</td>\n",
              "      <td>indices post biggest weekly decline june sense...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text       Date  Sentiment  \\\n",
              "0  Sebi to control unsolicited fin market advise ... 2023-02-24          1   \n",
              "1  Sebi pitches change in rules for REITs, InvITs... 2023-02-24          1   \n",
              "2  Sri Lanka's body approves renewable energy pro... 2023-02-24          1   \n",
              "3  Sebi likely to scrap small town-linked incenti... 2023-02-24          1   \n",
              "4  Indices post biggest weekly decline since June... 2023-02-24         -1   \n",
              "\n",
              "                                            Tok_text  \n",
              "0  sebi control unsolicited fin market advise soc...  \n",
              "1  sebi pitches change rules reits invits sponsor...  \n",
              "2  sri lanka body approves renewable energy proje...  \n",
              "3  sebi likely scrap small town linked incentive ...  \n",
              "4  indices post biggest weekly decline june sense...  "
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "punct = string.punctuation\n",
        "stopwords = list(STOP_WORDS)\n",
        "def text_data_cleaning(sentence):\n",
        "    sent = preprocess_text(sentence)\n",
        "    doc = nlp(sent)\n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.lemma_ != \"-PRON-\":\n",
        "            temp = token.lemma_.lower().strip()\n",
        "        else:\n",
        "            temp = token.lower_\n",
        "        tokens.append(temp)\n",
        "    \n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stopwords and token not in punct:\n",
        "            cleaned_tokens.append(token)\n",
        "    return append_message(cleaned_tokens)\n",
        "def preprocess_text(sen):\n",
        "    '''Cleans text data up, leaving only 2 or more char long non-stepwords composed of A-Z & a-z only\n",
        "    in lowercase'''\n",
        "    sentence = sen.lower()\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence) \n",
        "    # Remove multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)  \n",
        "    # Remove Stopwords\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
        "    sentence = pattern.sub('', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def append_message(text):\n",
        "  str = \" \"\n",
        "  return (str.join(text))\n",
        "\n",
        "\n",
        "df['Tok_text'] = df['Text'].apply(preprocess_text)\n",
        "\n",
        "tf_idf_vect = TfidfVectorizer()\n",
        "X = df['Tok_text']\n",
        "y = df['Sentiment']\n",
        "# X = tf_idf_vect.fit_transform(X)\n",
        "\n",
        "##splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "X_train = tf_idf_vect.fit_transform(X_train)\n",
        "X_test = tf_idf_vect.transform(X_test)\n",
        "print(X_train.shape, X_test.shape)\n",
        "## test data preprocessing\n",
        "test_data['Tok_text'] = test_data['Text'].apply(preprocess_text)\n",
        "test = test_data['Tok_text']\n",
        "x_test = tf_idf_vect.transform(test_data['Tok_text'])\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXC_kuDVpfks"
      },
      "source": [
        "**LinearSVC**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "nC6FiKJRQe54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Score: 0.7388924336098215\n",
            "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "SVC(C=10, gamma=0.1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.74      0.79      0.77        92\n",
            "           0       0.75      0.56      0.64        89\n",
            "           1       0.81      0.87      0.84       197\n",
            "\n",
            "    accuracy                           0.78       378\n",
            "   macro avg       0.77      0.74      0.75       378\n",
            "weighted avg       0.78      0.78      0.78       378\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier = SVC() \n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001,\"auto\"],\n",
        "              'kernel': ['rbf'],\n",
        "              } \n",
        "  \n",
        "grid = GridSearchCV(classifier, param_grid, refit = True, verbose = 0)\n",
        "# fitting the model for grid search\n",
        "grid.fit(X_train, y_train)\n",
        "print('Best Score: %s' % grid.best_score_)\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = grid.predict(x_test)\n",
        "check = pd.DataFrame({'Text':test_data['Text'],'Date':test_data['Date'],'Sentiment':y_pred})\n",
        "check.to_csv('check.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHwIT5p1gw43"
      },
      "source": [
        "SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "_62toT-1iPB6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Score: 0.7494972607863413\n",
            "{'alpha': 0.0001, 'loss': 'huber', 'n_jobs': -1, 'penalty': 'l2'}\n",
            "SGDClassifier(loss='huber', n_jobs=-1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.73      0.70      0.71        92\n",
            "           0       0.68      0.49      0.57        89\n",
            "           1       0.78      0.89      0.83       197\n",
            "\n",
            "    accuracy                           0.75       378\n",
            "   macro avg       0.73      0.69      0.71       378\n",
            "weighted avg       0.74      0.75      0.74       378\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier = SGDClassifier()\n",
        "param_grid = {\n",
        "    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate # number of epochs\n",
        "    'loss': ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber'], # logistic regression,\n",
        "    'penalty': ['l2'],\n",
        "    'n_jobs': [-1]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(classifier, param_grid, refit = True, verbose = 0)\n",
        "# fitting the model for grid search\n",
        "grid.fit(X_train, y_train)\n",
        "print('Best Score: %s' % grid.best_score_)\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = grid.predict(x_test)\n",
        "check = pd.DataFrame({'Text':test_data['Text'],'Date':test_data['Date'],'Sentiment':y_pred})\n",
        "check.to_csv('check.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uKhfs0Sj6a3"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "acqUqHOoj5X6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Score: 0.743530395370839\n",
            "{'C': 10, 'multi_class': 'multinomial', 'n_jobs': -1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "LogisticRegression(C=10, multi_class='multinomial', n_jobs=-1)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.75      0.74      0.74        92\n",
            "           0       0.68      0.56      0.62        89\n",
            "           1       0.81      0.88      0.85       197\n",
            "\n",
            "    accuracy                           0.77       378\n",
            "   macro avg       0.75      0.73      0.74       378\n",
            "weighted avg       0.77      0.77      0.77       378\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier = LogisticRegression()\n",
        "param_grid = {\n",
        "    'C': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 1e0],\n",
        "    'penalty': ['l2'],\n",
        "    'n_jobs': [-1],\n",
        "    'multi_class': ['multinomial'],\n",
        "    'solver': ['lbfgs']\n",
        "}\n",
        "grid = GridSearchCV(classifier, param_grid, refit = True, verbose = 0)\n",
        "# fitting the model for grid search\n",
        "grid.fit(X_train, y_train)\n",
        "print('Best Score: %s' % grid.best_score_)\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = grid.predict(x_test)\n",
        "check = pd.DataFrame({'Text':test_data['Text'],'Date':test_data['Date'],'Sentiment':y_pred})\n",
        "check.to_csv('check.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGZOgtX8gsqS"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "SAIyqU-nAg8y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Score: 0.7011875843454791\n",
            "{'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100, 'n_jobs': -1, 'random_state': 42}\n",
            "RandomForestClassifier(n_jobs=-1, random_state=42)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.68      0.72      0.70        76\n",
            "           0       0.84      0.50      0.63        96\n",
            "           1       0.75      0.89      0.82       186\n",
            "\n",
            "    accuracy                           0.75       358\n",
            "   macro avg       0.76      0.71      0.72       358\n",
            "weighted avg       0.76      0.75      0.74       358\n",
            "\n"
          ]
        }
      ],
      "source": [
        "classifier = RandomForestClassifier()\n",
        "param_grid = {\n",
        "    'min_samples_leaf': [1,3,5,10,30,45,50],\n",
        "    'max_features': ['sqrt', 'log2', 0.4, 0.5,],\n",
        "    'n_estimators': [100],\n",
        "    'n_jobs': [-1],\n",
        "    'random_state': [42]\n",
        "}\n",
        "grid = GridSearchCV(classifier, param_grid, refit = True, verbose = 0)\n",
        "# fitting the model for grid search\n",
        "grid.fit(X_train, y_train)\n",
        "print('Best Score: %s' % grid.best_score_)\n",
        "# print best parameter after tuning\n",
        "print(grid.best_params_)\n",
        "# print how our model looks after hyper-parameter tuning\n",
        "print(grid.best_estimator_)\n",
        "\n",
        "y_pred = grid.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = grid.predict(x_test)\n",
        "check = pd.DataFrame({'Text':test_data['Text'],'Date':test_data['Date'],'Sentiment':y_pred})\n",
        "check.to_csv('check.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7aRnGH_3EFD"
      },
      "source": [
        "Using CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Date</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Despite buoyancy in market, stock trading acti...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Markets snap 4-day rally; Bharti Airtel slumps...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sebi pegs dues worth Rs 67,228 cr as 'difficul...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stock of this IT enabled services company has ...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Amid heavy sell-off, FPIs dump 30 mn Paytm sha...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ITC hits new high in a subdued market; zooms 6...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LIC, Indiabulls Housing Finance struggle on ch...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Gold firms as dollar softens with market focus...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Cochin Shipyard, Mazagon Dock soar up to 8% on...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Max Healthcare rallies 5%; hits record high on...</td>\n",
              "      <td>2022-11-02</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text        Date  Sentiment\n",
              "0  Despite buoyancy in market, stock trading acti...  2022-11-02          0\n",
              "1  Markets snap 4-day rally; Bharti Airtel slumps...  2022-11-02         -1\n",
              "2  Sebi pegs dues worth Rs 67,228 cr as 'difficul...  2022-11-02          0\n",
              "3  Stock of this IT enabled services company has ...  2022-11-02          1\n",
              "4  Amid heavy sell-off, FPIs dump 30 mn Paytm sha...  2022-11-02         -1\n",
              "5  ITC hits new high in a subdued market; zooms 6...  2022-11-02          1\n",
              "6  LIC, Indiabulls Housing Finance struggle on ch...  2022-11-02         -1\n",
              "7  Gold firms as dollar softens with market focus...  2022-11-02          0\n",
              "8  Cochin Shipyard, Mazagon Dock soar up to 8% on...  2022-11-02          1\n",
              "9  Max Healthcare rallies 5%; hits record high on...  2022-11-02          1"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df1 = pd.read_csv('check.csv')\n",
        "df1.drop(columns=['Unnamed: 0'], axis=1,  inplace=True)\n",
        "df1.rename(columns={'0' : 'Sentiment'},inplace=True)\n",
        "index_name = df1[df1['Sentiment'].isna()==True].index\n",
        "df1.drop(index_name,inplace=True)\n",
        "df1.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Text       100 non-null    object\n",
            " 1   Date       100 non-null    object\n",
            " 2   Sentiment  100 non-null    int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 2.5+ KB\n"
          ]
        }
      ],
      "source": [
        "print(df1.info())\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1887 entries, 0 to 1886\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Text       1887 non-null   object\n",
            " 1   Date       1887 non-null   object\n",
            " 2   Sentiment  1887 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 44.4+ KB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kings\\AppData\\Local\\Temp\\ipykernel_7224\\261602568.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  fg = df.append(df1,ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "df.drop(columns=['Tok_text'], axis=1,  inplace=True)\n",
        "fg = df.append(df1,ignore_index=True)\n",
        "fg.info()\n",
        "fg.to_\n",
        "fg.to_csv('stock_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df2 = test_data[['Text','Date']].merge(df1, on=['Text'], how='left')\n",
        "# index_name = df2[df2['Sentiment'].isna()==True].index\n",
        "# df2.drop(index_name,inplace=True)\n",
        "# df2.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOzc0IBWtr-a"
      },
      "outputs": [],
      "source": [
        "# pos_msg = df[df['Sentiment'] == 1]\n",
        "# zeo_msg = df[df['Sentiment']==0]\n",
        "# nrg_msg = df[df['Sentiment']==-1]\n",
        "# pos_msg_text = \" \".join(pos_msg.Tok_text.to_numpy().tolist())\n",
        "# zeo_msg_text = \" \".join(zeo_msg.Tok_text.to_numpy().tolist())\n",
        "# nrg_msg_text = \" \".join(nrg_msg.Tok_text.to_numpy().tolist())\n",
        "\n",
        "# pos_msg_cloud = WordCloud(width =520, height =260, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(pos_msg_text)\n",
        "# plt.figure(figsize=(16,10))\n",
        "# plt.imshow(pos_msg_cloud, interpolation='bilinear')\n",
        "# plt.axis('off') # turn off axis\n",
        "# plt.show()\n",
        "# zeo_msg_cloud = WordCloud(width =520, height =260, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(zeo_msg_text)\n",
        "# plt.figure(figsize=(16,10))\n",
        "# plt.imshow(zeo_msg_cloud, interpolation='bilinear')\n",
        "# plt.axis('off') # turn off axis\n",
        "# plt.show()\n",
        "# nrg_msg_cloud = WordCloud(width =520, height =260, stopwords=STOPWORDS,max_font_size=50, background_color =\"black\", colormap='Blues').generate(nrg_msg_text)\n",
        "# plt.figure(figsize=(16,10))\n",
        "# plt.imshow(nrg_msg_cloud, interpolation='bilinear')\n",
        "# plt.axis('off') # turn off axis\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NeynTHpSsYR"
      },
      "outputs": [],
      "source": [
        "# fix random seed for reproducibility\n",
        "jj\n",
        "tf.random.set_seed(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "\n",
        "top_words = 5000\n",
        "#(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "X = []\n",
        "y = df['Sentiment']\n",
        "sentences = list(df['Text'])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(X_train)\n",
        "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
        "X_test = word_tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "vocab_length = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/drive/My Drive/Colab Notebooks/a2_glove.6B.100d.txt', encoding=\"utf8\")#/content/drive/MyDrive/Colab Notebooks/a2_glove.6B.100d.txt\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "\n",
        "embedding_matrix = zeros((vocab_length, 100))\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6f0PEILvYpJ"
      },
      "outputs": [],
      "source": [
        "# Neural Network architecture\n",
        "\n",
        "cnn_model = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "cnn_model.add(embedding_layer)\n",
        "\n",
        "cnn_model.add(Conv1D(128, 5, activation='relu'))\n",
        "cnn_model.add(GlobalMaxPooling1D())\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUBLeVUCva_n"
      },
      "outputs": [],
      "source": [
        "# Model compiling\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(cnn_model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6Y6Ki_MvcwT"
      },
      "outputs": [],
      "source": [
        "cnn_model_history = cnn_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1Z0mZMjvfPr"
      },
      "outputs": [],
      "source": [
        "score = cnn_model.evaluate(X_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmYvcBD8vvpK"
      },
      "outputs": [],
      "source": [
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S9gqfPA45oF"
      },
      "outputs": [],
      "source": [
        "def build_model(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Dense(\n",
        "      hp.Choice('units', [8, 16, 32]),\n",
        "      activation='relu'))\n",
        "  model.add(keras.layers.Dense(1, activation='relu'))\n",
        "  model.compile(loss='mse')\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUO6s3B05VVz"
      },
      "outputs": [],
      "source": [
        "tuner = keras_tuner.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rKLMDoE5X4E"
      },
      "outputs": [],
      "source": [
        "tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "best_model = tuner.get_best_models()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnCO2JIM9CBs"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdmnFwdq_fhK"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9flQzdny9Jk4"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.0002, \n",
        "    epsilon=1e-08, \n",
        "    clipnorm=1.0),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FuEruMjAFpl"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, epochs=2, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG25WZnlArBH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wRKqtbWUldM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AZBHiXEUlZ9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df078hbRUlXg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMuwdS-TUlVD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZfA7q06UlSO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCuTRWMMUlPa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9NjhO3IUlKa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1o2M4iMUmP5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqPlYx-kWP-h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
