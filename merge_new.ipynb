{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 18, 2023, Saturday\n",
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "descriptor 'strftime' for 'datetime.date' objects doesn't apply to a 'str' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(date_sto)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(date_sto))\n\u001b[1;32m---> 14\u001b[0m dat \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39;49mstrftime(\u001b[39mstr\u001b[39;49m(date_sto),\u001b[39m'\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mB \u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m, \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY, \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mdate()\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(dat)\n\u001b[0;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(dat))\n",
      "\u001b[1;31mTypeError\u001b[0m: descriptor 'strftime' for 'datetime.date' objects doesn't apply to a 'str' object"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,date,timedelta\n",
    "# today = date.today() - timedelta(1)\n",
    "# today = str(today) + ' 09:00 PM'\n",
    "# stop_date = datetime.strptime(today, '%Y-%m-%d %I:%M %p')\n",
    "# print(stop_date)\n",
    "# t = datetime.now().date()\n",
    "# print(str(t.strftime('%B %d, %Y, %A')))\n",
    "\n",
    "date_stop = datetime.now().date()\n",
    "\n",
    "date_sto = date_stop.strftime('%B %d, %Y, %A')\n",
    "print(date_sto)\n",
    "print(type(date_sto))\n",
    "dat = datetime.strftime(str(date_sto),'%B %d, %Y, %A').date()\n",
    "print(dat)\n",
    "print(type(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 3, 5, 0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.strptime(\"March 05, 2023, Sunday\", '%B %d, %Y, %A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw_news/*.csv\n",
      "./raw_news/*.csv\n",
      "       Unnamed: 0                                           Headline  \\\n",
      "0               0  Sebi to control unsolicited fin market advise ...   \n",
      "1               1  Sebi pitches change in rules for REITs, InvITs...   \n",
      "2               2  Sri Lanka's body approves renewable energy pro...   \n",
      "3               3  Sebi likely to scrap small town-linked incenti...   \n",
      "4               4  Indices post biggest weekly decline since June...   \n",
      "...           ...                                                ...   \n",
      "31500          24  LIC's investment in Adani Group stocks now sta...   \n",
      "31501          25  PolicyBazaar hits over 5-month high; surges 30...   \n",
      "31502          26  Jindal Stainless (Hisar) fixes record date for...   \n",
      "31503          27  Mahindra Lifespace tumbles 8% after MD & CEO t...   \n",
      "31504          28  Siemens scaling new heights on strong performa...   \n",
      "\n",
      "                            Date  \n",
      "0      February 24, 2023, Friday  \n",
      "1      February 24, 2023, Friday  \n",
      "2      February 24, 2023, Friday  \n",
      "3      February 24, 2023, Friday  \n",
      "4      February 24, 2023, Friday  \n",
      "...                          ...  \n",
      "31500  February 24, 2023, Friday  \n",
      "31501  February 24, 2023, Friday  \n",
      "31502  February 24, 2023, Friday  \n",
      "31503  February 24, 2023, Friday  \n",
      "31504  February 24, 2023, Friday  \n",
      "\n",
      "[31505 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# merging the files\n",
    "joined_files = os.path.join(\"./raw_news/\", \"*.csv\")\n",
    "print(joined_files)\n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "print(joined_files)\n",
    "# Finally, the files are joined\n",
    "df = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sebi to control unsolicited fin market advise ...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sebi pitches change in rules for REITs, InvITs...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sri Lanka's body approves renewable energy pro...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sebi likely to scrap small town-linked incenti...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Indices post biggest weekly decline since June...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Headline  \\\n",
       "0           0  Sebi to control unsolicited fin market advise ...   \n",
       "1           1  Sebi pitches change in rules for REITs, InvITs...   \n",
       "2           2  Sri Lanka's body approves renewable energy pro...   \n",
       "3           3  Sebi likely to scrap small town-linked incenti...   \n",
       "4           4  Indices post biggest weekly decline since June...   \n",
       "\n",
       "                        Date  \n",
       "0  February 24, 2023, Friday  \n",
       "1  February 24, 2023, Friday  \n",
       "2  February 24, 2023, Friday  \n",
       "3  February 24, 2023, Friday  \n",
       "4  February 24, 2023, Friday  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31505 entries, 0 to 31504\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Headline  31505 non-null  object\n",
      " 1   Date      31505 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 492.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = np.random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sebi to control unsolicited fin market advise ...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sebi pitches change in rules for REITs, InvITs...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sri Lanka's body approves renewable energy pro...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sebi likely to scrap small town-linked incenti...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indices post biggest weekly decline since June...</td>\n",
       "      <td>February 24, 2023, Friday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  Sebi to control unsolicited fin market advise ...   \n",
       "1  Sebi pitches change in rules for REITs, InvITs...   \n",
       "2  Sri Lanka's body approves renewable energy pro...   \n",
       "3  Sebi likely to scrap small town-linked incenti...   \n",
       "4  Indices post biggest weekly decline since June...   \n",
       "\n",
       "                        Date  Sentiment  \n",
       "0  February 24, 2023, Friday          0  \n",
       "1  February 24, 2023, Friday          0  \n",
       "2  February 24, 2023, Friday          0  \n",
       "3  February 24, 2023, Friday          0  \n",
       "4  February 24, 2023, Friday          0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./news/stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\kings\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kings\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kings\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kings\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kings\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment: 43.43%\n",
      "Neutral Sentiment: 40.80%\n",
      "Negative Sentiment: 15.77%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# Load stock news data\n",
    "df = pd.read_csv('stock_data.csv')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Add sentiment scores to dataframe\n",
    "df['sentiment'] = df['Text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Categorize sentiment\n",
    "df.loc[df['sentiment'] > 0, 'sentiment_category'] = 'Positive'\n",
    "df.loc[df['sentiment'] == 0, 'sentiment_category'] = 'Neutral'\n",
    "df.loc[df['sentiment'] < 0, 'sentiment_category'] = 'Negative'\n",
    "\n",
    "# Calculate percentage of positive, neutral, and negative sentiment\n",
    "sentiment_counts = df['sentiment_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print sentiment percentages\n",
    "print(\"Positive Sentiment: {:.2f}%\".format(sentiment_counts['Positive']))\n",
    "print(\"Neutral Sentiment: {:.2f}%\".format(sentiment_counts['Neutral']))\n",
    "print(\"Negative Sentiment: {:.2f}%\".format(sentiment_counts['Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('ch.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "data = pd.read_csv('stock_data.csv')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_data = tokenizer(dat[\"sentence\"], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = np.array(dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 'Ramya1', '21-01-2022', 1)\n",
      "[(6, 'Ramya2', '21-01-2022', 1), (7, 'Ramya22', '21-01-2022', 1), (8, 'Ramys2', '21-01-2022', 1), (9, 'Ramys2', '21-01-2022', 1), (10, 'Ramys2', '21-01-2022', 1)]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "#Establishing the connection\n",
    "conn = psycopg2.connect(\n",
    "   database=\"stocks\", user='postgres', password='kingsuk', host='127.0.0.1', port= '5432'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "hs = 'hsjs'\n",
    "cursor.execute('''INSERT INTO NEWS(news, news_date, sentiment) VALUES ('Ramys2', '21-01-2022', 1)''')\n",
    "conn.commit()\n",
    "#Creating a cursor object using the cursor() method\n",
    "cursor.execute('''SELECT * from NEWS''')\n",
    "\n",
    "#Fetching 1st row from the table\n",
    "result = cursor.fetchone();\n",
    "print(result)\n",
    "\n",
    "#Fetching 1st row from the table\n",
    "result = cursor.fetchall();\n",
    "print(result)\n",
    "\n",
    "\n",
    "#Closing the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kings\\AppData\\Local\\Temp\\ipykernel_14436\\1611249869.py:11: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list1 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:TCS').get_items()):\n",
    "    if i>100:\n",
    "        break\n",
    "    tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "    \n",
    "# Creating a dataframe from the tweets list above \n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-03-20 08:33:14+0000', tz='UTC')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df1['Datetime'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sntwitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m tweets_list2 \u001b[39m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[39m# Using TwitterSearchScraper to scrape data and append tweets to list\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m i,tweet \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sntwitter\u001b[39m.\u001b[39mTwitterSearchScraper(\u001b[39m'\u001b[39m\u001b[39mTCS since:2023-03-25 until:2023-03-26\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mget_items()):\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m>\u001b[39mmaxTweets:\n\u001b[0;32m      9\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sntwitter' is not defined"
     ]
    }
   ],
   "source": [
    "maxTweets = 500\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('TCS since:2023-03-25 until:2023-03-26').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets_df2\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_df2' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tweets_df2\u001b[39m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tweets_df2' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n\u001b[0;32m      4\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(df[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mSentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mSentiment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint64)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39minfo())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def filter_news(news_data, company_filter):\n",
    "    news_data['Text'] = news_data['Text'].apply(str.lower)\n",
    "    company_filter['Symbol'] = company_filter['Symbol'].apply(str).apply(str.lower)\n",
    "    company_filter['Company Name'] = company_filter['Company Name'].apply(str).apply(str.lower)\n",
    "    final_data = pd.DataFrame()\n",
    "    news_data['Text'] = news_data['Text'].str.replace('[^\\w\\s]', ' ')\n",
    "    news_data['Text'] = \" \"+news_data['Text']+\" \"\n",
    "\n",
    "    # iterate all company names to find out the match\n",
    "    for i in range(len(company_filter)-1):\n",
    "        try:\n",
    "            keyword = \" \"+company_filter[\"Symbol\"][i]+\" \"\n",
    "            CompanyName = \" \"+company_filter[\"Company Name\"][i]+\" \"\n",
    "            l=[\" global \",\" wealth \",\" take \",\" focus \",\" worth \"]\n",
    "            if(keyword == 'ttl'):\n",
    "                continue\n",
    "            \n",
    "            if(keyword in l):\n",
    "                print(keyword)\n",
    "                contain_values = news_data[(news_data['Text'].str.contains(keyword))&(news_data['Text'].str.contains(CompanyName))]\n",
    "                contain_values['Symbol'] = company_filter[\"Symbol\"][i]\n",
    "                contain_values['Company Name'] = company_filter[\"Company Name\"][i]\n",
    "                final_data = final_data.append(contain_values, ignore_index=True)\n",
    "\n",
    "            else:\n",
    "                contain_values = news_data[(news_data['Text'].str.contains(keyword))|(news_data['Text'].str.contains(CompanyName))]\n",
    "                contain_values['Symbol'] = company_filter[\"Symbol\"][i]\n",
    "                contain_values['Company Name'] = company_filter[\"Company Name\"][i]\n",
    "                final_data = final_data.append(contain_values, ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            print(\"Company name is small at index \", l)\n",
    "\n",
    "    return final_data\n",
    "data1['Headline']=data1['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 32658 entries, 0 to 32657\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Text       32658 non-null  object        \n",
      " 1   Date       32658 non-null  datetime64[ns]\n",
      " 2   Sentiment  32658 non-null  int64         \n",
      " 3   Tok_text   32656 non-null  object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(2)\n",
      "memory usage: 1.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('stock_data.csv')\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df.drop_duplicates(subset=\"Text\",\n",
    "                     keep=False, inplace=True)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Sentiment'] = df['Sentiment'].astype(np.int64)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52093 entries, 0 to 52092\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   Unnamed: 0  52093 non-null  int64         \n",
      " 1   Text        52093 non-null  object        \n",
      " 2   Date        52093 non-null  datetime64[ns]\n",
      " 3   Sentiment   52093 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(2), object(1)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D, Bidirectional,GRU,Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam  \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "df = pd.read_csv('stock_data.csv')\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Sentiment'] = df['Sentiment'].astype(np.int64)\n",
    "print(df.info())\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "to_remove = ['up', 'down','low','high','below','less','fall']\n",
    "new_stopwords = set(stopwords.words('english')).difference(to_remove)\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "def text_data_cleaning(sentence):\n",
    "    sent = preprocess_text(sentence)\n",
    "    doc = nltk.word_tokenize(sent)\n",
    "    lemma =[stemmer.stem(word) for word in doc]\n",
    "    return append_message(lemma)\n",
    "def preprocess_text(sen):\n",
    "    '''Cleans text data up, leaving only 2 or more char long non-stepwords composed of A-Z & a-z only\n",
    "    in lowercase'''\n",
    "    sentence = sen.lower()\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = re.sub('Stocks|market', ' ', sentence)\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence) \n",
    "    # Remove multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)  \n",
    "    # Remove Stopwords\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(new_stopwords) + r')\\b\\s*')\n",
    "    sentence = pattern.sub('', sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def append_message(text):\n",
    "  str = \" \"\n",
    "  return (str.join(text))\n",
    "\n",
    "df['Tok_text'] = df['Text'].apply(text_data_cleaning)\n",
    "df['Tok_text'] = df['Tok_text'].astype(str)\n",
    "\n",
    "df = df.drop_duplicates(subset='Tok_text',keep='first')\n",
    "df2 = df.iloc[:,2:]\n",
    "\n",
    "X = df2['Tok_text']\n",
    "y= df2['Sentiment']\n",
    "\n",
    "df_train, df_test = train_test_split(df2, test_size=0.3, random_state=42,\n",
    "                                     stratify=df2['Sentiment'])\n",
    "df_test, df_val = train_test_split(df_test, test_size=0.5, random_state=42,\n",
    "                                     stratify=df_test['Sentiment'])\n",
    "\n",
    "vocab = set()\n",
    "for x in X:\n",
    "    vocab.add(x)\n",
    "tokenizer = Tokenizer(num_words=len(vocab))\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "max_length = max(len(sequence) for sequence in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.3, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "pickle.dump(sequences, open(\"sequences.pkl\", \"wb\"))\n",
    "pickle.dump(max_length, open(\"max_length.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "13/13 [==============================] - 18s 1s/step - loss: 1.0194 - sparse_categorical_accuracy: 0.4958 - val_loss: 0.8610 - val_sparse_categorical_accuracy: 0.6203\n",
      "Epoch 2/5\n",
      "13/13 [==============================] - 16s 1s/step - loss: 0.6971 - sparse_categorical_accuracy: 0.7145 - val_loss: 0.5028 - val_sparse_categorical_accuracy: 0.8164\n",
      "Epoch 3/5\n",
      "13/13 [==============================] - 17s 1s/step - loss: 0.4160 - sparse_categorical_accuracy: 0.8472 - val_loss: 0.3693 - val_sparse_categorical_accuracy: 0.8584\n",
      "Epoch 4/5\n",
      "13/13 [==============================] - 17s 1s/step - loss: 0.2975 - sparse_categorical_accuracy: 0.8959 - val_loss: 0.3494 - val_sparse_categorical_accuracy: 0.8713\n",
      "Epoch 5/5\n",
      "13/13 [==============================] - 16s 1s/step - loss: 0.2549 - sparse_categorical_accuracy: 0.9140 - val_loss: 0.3572 - val_sparse_categorical_accuracy: 0.8696\n",
      "485/485 [==============================] - 2s 5ms/step - loss: 0.3572 - sparse_categorical_accuracy: 0.8696\n",
      "Test loss: 0.3571629226207733\n",
      "485/485 [==============================] - 3s 5ms/step\n",
      "Score for label 0: 84.16%\n",
      "Score for label 1: 83.95%\n",
      "Score for label 2: 79.52%\n"
     ]
    }
   ],
   "source": [
    "# Define model architecture\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab), 150, input_length=max_length))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "optimizer = Adam(\n",
    "    learning_rate=2e-03, # HF recommendation\n",
    "    epsilon=1e-08,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=3000)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "y_pred = model.predict(X_test)\n",
    "label_scores = {}\n",
    "for i, label in enumerate(set(y)):\n",
    "    idx = np.where(y_test == i)[0]\n",
    "    label_scores[label] = np.mean(y_pred[idx, i])\n",
    "\n",
    "# print the label specific score\n",
    "for label in label_scores:\n",
    "    print('Score for label {}: {:.2f}%'.format(label, label_scores[label] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekday (index): 5\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.date.today().weekday()\n",
    "\n",
    "# Print the weekday and weekday name\n",
    "print(\"Weekday (index):\", current_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "13/13 [==============================] - 26s 2s/step - loss: 1.0368 - sparse_categorical_accuracy: 0.4920 - val_loss: 0.9820 - val_sparse_categorical_accuracy: 0.5085\n",
      "Epoch 2/8\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.8973 - sparse_categorical_accuracy: 0.5764 - val_loss: 0.7684 - val_sparse_categorical_accuracy: 0.6617\n",
      "Epoch 3/8\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.6827 - sparse_categorical_accuracy: 0.7177 - val_loss: 0.5741 - val_sparse_categorical_accuracy: 0.7883\n",
      "Epoch 4/8\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.4986 - sparse_categorical_accuracy: 0.8106 - val_loss: 0.4388 - val_sparse_categorical_accuracy: 0.8341\n",
      "Epoch 5/8\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.3857 - sparse_categorical_accuracy: 0.8570 - val_loss: 0.3823 - val_sparse_categorical_accuracy: 0.8551\n",
      "Epoch 6/8\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.3234 - sparse_categorical_accuracy: 0.8809 - val_loss: 0.3497 - val_sparse_categorical_accuracy: 0.8676\n",
      "Epoch 7/8\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.2857 - sparse_categorical_accuracy: 0.8963 - val_loss: 0.3373 - val_sparse_categorical_accuracy: 0.8741\n",
      "Epoch 8/8\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.2600 - sparse_categorical_accuracy: 0.9060 - val_loss: 0.3352 - val_sparse_categorical_accuracy: 0.8769\n",
      "485/485 [==============================] - 6s 11ms/step - loss: 0.3352 - sparse_categorical_accuracy: 0.8769\n",
      "Test loss: 0.33523350954055786\n",
      "485/485 [==============================] - 6s 11ms/step\n",
      "Score for label 0: 76.67%\n",
      "Score for label 1: 87.41%\n",
      "Score for label 2: 79.40%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab), 128, input_length=max_length))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Bidirectional(GRU(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(len(set(y)), activation='softmax'))\n",
    "optimizer = Adam(\n",
    "    learning_rate=1e-03, # HF recommendation\n",
    "    epsilon=1e-08,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=3000)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "y_pred = model.predict(X_test)\n",
    "label_scores = {}\n",
    "for i, label in enumerate(set(y)):\n",
    "    idx = np.where(y_test == i)[0]\n",
    "    label_scores[label] = np.mean(y_pred[idx, i])\n",
    "\n",
    "# print the label specific score\n",
    "for label in label_scores:\n",
    "    print('Score for label {}: {:.2f}%'.format(label, label_scores[label] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('biGRU.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "13/13 [==============================] - 31s 2s/step - loss: 1.0340 - sparse_categorical_accuracy: 0.4974 - val_loss: 0.9391 - val_sparse_categorical_accuracy: 0.5093\n",
      "Epoch 2/8\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.8343 - sparse_categorical_accuracy: 0.6096 - val_loss: 0.6558 - val_sparse_categorical_accuracy: 0.7036\n",
      "Epoch 3/8\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.5852 - sparse_categorical_accuracy: 0.7691 - val_loss: 0.4126 - val_sparse_categorical_accuracy: 0.8456\n",
      "Epoch 4/8\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.4231 - sparse_categorical_accuracy: 0.8445 - val_loss: 0.3584 - val_sparse_categorical_accuracy: 0.8665\n",
      "Epoch 5/8\n",
      "13/13 [==============================] - 26s 2s/step - loss: 0.3611 - sparse_categorical_accuracy: 0.8702 - val_loss: 0.3625 - val_sparse_categorical_accuracy: 0.8662\n",
      "Epoch 6/8\n",
      "13/13 [==============================] - 25s 2s/step - loss: 0.3322 - sparse_categorical_accuracy: 0.8795 - val_loss: 0.3408 - val_sparse_categorical_accuracy: 0.8713\n",
      "Epoch 7/8\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.3061 - sparse_categorical_accuracy: 0.8904 - val_loss: 0.3332 - val_sparse_categorical_accuracy: 0.8773\n",
      "Epoch 8/8\n",
      "13/13 [==============================] - 23s 2s/step - loss: 0.2911 - sparse_categorical_accuracy: 0.8974 - val_loss: 0.3288 - val_sparse_categorical_accuracy: 0.8775\n",
      "485/485 [==============================] - 6s 12ms/step - loss: 0.3288 - sparse_categorical_accuracy: 0.8775\n",
      "Test loss: 0.3287879526615143\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000125CDD2DE10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 15:07:55 [WARNING] 5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000125CDD2DE10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 6s 12ms/step\n",
      "Score for label 0: 79.01%\n",
      "Score for label 1: 86.02%\n",
      "Score for label 2: 81.38%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab), 128, input_length=max_length))\n",
    "model.add(SpatialDropout1D(0.5))\n",
    "model.add(Bidirectional(GRU(128, dropout=0.5, recurrent_dropout=0.5)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(set(y)), activation='softmax'))\n",
    "optimizer = Adam(\n",
    "    learning_rate=2e-03, # HF recommendation\n",
    "    epsilon=1e-08,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=8, batch_size=3000)\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}')\n",
    "y_pred = model.predict(X_test)\n",
    "label_scores = {}\n",
    "for i, label in enumerate(set(y)):\n",
    "    idx = np.where(y_test == i)[0]\n",
    "    label_scores[label] = np.mean(y_pred[idx, i])\n",
    "\n",
    "# print the label specific score\n",
    "for label in label_scores:\n",
    "    print('Score for label {}: {:.2f}%'.format(label, label_scores[label] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9904066e-01, 9.4413193e-04, 1.5094623e-05], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = y_pred[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99904066"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = d.max()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ass = np.where(d == a)[0][0]\n",
    "print(ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('biGRU1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 15:20:17 [INFO] fetching from money control\n",
      "2023-05-06 15:20:17 [INFO] fetching from mint\n",
      "2023-05-06 15:20:17 [INFO] fetching from business standard\n",
      "2023-05-06 15:20:17 [INFO] fetching from economic times\n",
      "2023-05-06 15:20:17 [INFO] fetch from https://www.business-standard.com/markets/news\n",
      "2023-05-06 15:20:17 [INFO] fetching completed from business standard\n",
      "2023-05-06 15:20:18 [INFO] fetch from https://economictimes.indiatimes.com/markets/stocks/news/articlelist/msid-2146843,page-1.cms\n",
      "2023-05-06 15:20:18 [INFO] fetch from https://www.moneycontrol.com/news/business/markets/page-1\n",
      "2023-05-06 15:20:18 [INFO] fetch from https://www.livemint.com/market/stock-market-news/page-1\n",
      "2023-05-06 15:20:18 [INFO] fetching completed from money control\n",
      "2023-05-06 15:20:18 [INFO] fetching completed from mint\n",
      "2023-05-06 15:20:18 [INFO] fetch from https://economictimes.indiatimes.com/markets/stocks/news/articlelist/msid-2146843,page-2.cms\n",
      "2023-05-06 15:20:18 [INFO] fetching comepleted from economic times\n",
      "2023-05-06 15:20:18 [INFO] dataframe created\n",
      "2023-05-06 15:20:18 [INFO] stopwords removed and lematization completed\n",
      "2023-05-06 15:20:18 [INFO] model loaded\n",
      "2023-05-06 15:20:18 [INFO] data predicted\n",
      "2023-05-06 15:20:18 [INFO] lstm model loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 15:20:19 [INFO] data predicted\n",
      "2023-05-06 15:20:20 [INFO] GRU model loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 15:20:20 [INFO] data predicted\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level to INFO (or any desired level)\n",
    "    # Set the log message format\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'  # Set the date and time format\n",
    ")\n",
    "\n",
    "\n",
    "mcnews = []\n",
    "mctimes = []\n",
    "mnews = []\n",
    "mtimes = []\n",
    "bsnews = []\n",
    "bstimes = []\n",
    "ecnews = []\n",
    "ectimes = []\n",
    "\n",
    "\n",
    "today = date.today() - timedelta(1)\n",
    "today = str(today) + ' 09:00 PM'\n",
    "stop_date = datetime.strptime(today, '%Y-%m-%d %I:%M %p')\n",
    "\n",
    "\n",
    "def fetch_from_url(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    logging.info(\"fetch from \" + url)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def extract_data_from_page(page, ele, classname):\n",
    "    return page.find_all(ele, class_=classname)\n",
    "\n",
    "\n",
    "def economic_news():\n",
    "    today = datetime.now()\n",
    "    month = today.strftime(\"%b\")\n",
    "    logging.info(\"fetching from economic times\")\n",
    "    for x in range(1, 4):\n",
    "        url = \"https://economictimes.indiatimes.com/markets/stocks/news/articlelist/msid-2146843,\"\n",
    "        page = fetch_from_url(url + \"page-\" + str(x) + \".cms\")\n",
    "        result = extract_data_from_page(page, 'div', 'eachStory')\n",
    "        for i in result:\n",
    "            data = i.find('a')\n",
    "            times = i.find('time')\n",
    "            times = times.text\n",
    "            stop_time = datetime.strptime(\n",
    "                str(times).strip(), '%b %d, %Y, %I:%M %p IST')\n",
    "            data = data.text.strip()\n",
    "            if stop_time <= stop_date:\n",
    "                logging.info(\"fetching comepleted from economic times\")\n",
    "                return\n",
    "\n",
    "            ecnews.append(data)\n",
    "            ectimes.append(format_date(stop_time))\n",
    "\n",
    "\n",
    "def mint_news():\n",
    "    logging.info(\"fetching from mint\")\n",
    "    for x in range(1, 4):\n",
    "        url = \"https://www.livemint.com/market/stock-market-news/\"\n",
    "        page = fetch_from_url(url+\"page-\" + str(x))\n",
    "\n",
    "        result = extract_data_from_page(page, 'div', 'headlineSec')\n",
    "        for i in result:\n",
    "            data = i.find('a')\n",
    "            tim = i.find('span')\n",
    "\n",
    "            data = data.text.strip()\n",
    "            tim = \"\".join(\n",
    "                [str(x)[136:161] for x in i.contents if \"<span data-expandedtime\" in str(x)])\n",
    "\n",
    "            stop_time = datetime.strptime(str(tim), '%d %b %Y, %I:%M %p IST')\n",
    "            if stop_time <= stop_date:\n",
    "                logging.info(\"fetching completed from mint\")\n",
    "                return\n",
    "            mnews.append(data)\n",
    "            mtimes.append(format_date(stop_time))\n",
    "\n",
    "\n",
    "def money_control_news():\n",
    "    logging.info(\"fetching from money control\")\n",
    "    for x in range(1, 4):\n",
    "        url = \"https://www.moneycontrol.com/news/business/markets/\"\n",
    "        page = fetch_from_url(url+\"page-\" + str(x))\n",
    "\n",
    "        result = extract_data_from_page(page, 'li', 'clearfix')\n",
    "        for i in result:\n",
    "            stop_time = ''\n",
    "            page_text = (i.get_text()).strip()\n",
    "            page_text = str(page_text).replace(\n",
    "                i.find('p').text.strip(), \"\").strip()\n",
    "\n",
    "            page_text = page_text.split('IST')\n",
    "            stop_time = datetime.strptime(\n",
    "                str(page_text[0]).strip(), '%B %d, %Y %I:%M %p')\n",
    "            if stop_time <= stop_date:\n",
    "                logging.info(\"fetching completed from money control\")\n",
    "                return\n",
    "            mcnews.append(page_text[1])\n",
    "            mctimes.append(format_date(stop_time))\n",
    "\n",
    "\n",
    "def business_standard_news():\n",
    "    date_stop = datetime.now().date()\n",
    "    logging.info(\"fetching from business standard\")\n",
    "    url = \"https://www.business-standard.com/markets/news\"\n",
    "    page = fetch_from_url(url)\n",
    "    result = extract_data_from_page(page, 'div', 'cardlist')\n",
    "    for i in result:\n",
    "        data = i.find('a')\n",
    "        bsnews.append(data.text)\n",
    "        bstimes.append(date_stop)\n",
    "    logging.info(\"fetching completed from business standard\")\n",
    "\n",
    "\n",
    "def text_data_cleaning(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sent = preprocess_text(sentence)\n",
    "    doc = nltk.word_tokenize(sent)\n",
    "    lemma = [lemmatizer.lemmatize(word, pos=\"v\") for word in doc]\n",
    "    return append_message(lemma)\n",
    "\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    to_remove = ['up', 'down', 'low', 'high', 'below', 'less', 'fall']\n",
    "    new_stopwords = set(stopwords.words('english')).difference(to_remove)\n",
    "    sentence = str(sen).lower()\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = re.sub('rs|cr|crore|point|points|pt|stock', ' ', sentence)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(new_stopwords) + r')\\b\\s*')\n",
    "    sentence = pattern.sub('', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def append_message(text):\n",
    "    str = \" \"\n",
    "    return (str.join(text))\n",
    "\n",
    "\n",
    "def filter_data(data):\n",
    "    data = data[data['Text'].str.contains(\"\\?\") != True]\n",
    "    data = data[data['Text'].str.contains(\"Wall Street|Amazon|Google\") != True]\n",
    "    data['Tok_text'] = data['Text'].apply(text_data_cleaning)\n",
    "    logging.info(\"stopwords removed and lematization completed\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def ml_model(data):\n",
    "    model = pickle.load(open('model.pkl', 'rb'))\n",
    "    tfidf = pickle.load(open('vectorizer.pkl', 'rb'))\n",
    "    logging.info(\"model loaded\")\n",
    "    x_test = tfidf.transform(data['Tok_text'])\n",
    "    y_pred = model.predict(x_test)\n",
    "    logging.info(\"data predicted\")\n",
    "    data['lg_pred'] = y_pred\n",
    "    return data\n",
    "\n",
    "\n",
    "def dp_model(data):\n",
    "    tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))\n",
    "    max_length = pickle.load(open('max_length.pkl', 'rb'))\n",
    "    sequences = tokenizer.texts_to_sequences(data['Tok_text'])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "    lstm_model = load_model('./Lstm.h5')\n",
    "    logging.info(\"lstm model loaded\")\n",
    "    lstm_pred = lstm_model.predict(padded_sequences)\n",
    "    data['lstm_pred'] = np.argmax(lstm_pred, axis=1)\n",
    "    logging.info(\"data predicted\")\n",
    "    biGru_model = load_model('./biGRU.h5')\n",
    "    logging.info(\"GRU model loaded\")\n",
    "    biGru_pred = biGru_model.predict(padded_sequences)\n",
    "    data['biGru_pred'] = np.argmax(biGru_pred, axis=1)\n",
    "    logging.info(\"data predicted\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def format_date(timestamp):\n",
    "    dt = datetime.strptime(str(timestamp), '%Y-%m-%d %H:%M:%S')\n",
    "    formatted_date = dt.strftime('%Y-%m-%d')\n",
    "    return formatted_date\n",
    "\n",
    "\n",
    "def calculate_sentiment(data):\n",
    "    sentiment = data.iloc[-3:].tolist()\n",
    "    return max(sentiment,key=sentiment.count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    money_control = threading.Thread(\n",
    "        target=money_control_news, name=\"moneycontrol\")\n",
    "    mint = threading.Thread(target=mint_news, name=\"mint\")\n",
    "    business_standard = threading.Thread(\n",
    "        target=business_standard_news, name=\"business_standard\")\n",
    "    economic_times = threading.Thread(\n",
    "        target=economic_news, name=\"economic_times\")\n",
    "\n",
    "    money_control.start()\n",
    "    mint.start()\n",
    "    business_standard.start()\n",
    "    economic_times.start()\n",
    "\n",
    "    money_control.join()\n",
    "    mint.join()\n",
    "    business_standard.join()\n",
    "    economic_times.join()\n",
    "    news = mcnews+mnews+ecnews+bsnews\n",
    "    times = mctimes+mtimes+ectimes+bstimes\n",
    "    data = pd.DataFrame({'Text': news, 'Date': times})\n",
    "    logging.info(\"dataframe created\")\n",
    "    data['Date'] = pd.to_datetime(data['Date']).dt.date\n",
    "    data = filter_data(data)\n",
    "    data = ml_model(data)\n",
    "    data = dp_model(data)\n",
    "    data.drop(columns=['Tok_text'], inplace=True)\n",
    "    data['Sentiment']=data.apply(calculate_sentiment,axis=1)\n",
    "    data.drop(columns=['Tok_text','lstm_pred','biGru_pred','lg_pred'], inplace=True)\n",
    "    data.to_csv('test1.csv')\n",
    "\n",
    "    # repo = git.Repo('.')\n",
    "    # subprocess.check_output(\"git add .\", stderr=subprocess.PIPE)\n",
    "    # repo.index.commit('news fetch for ' + str(datetime.date.today()))\n",
    "    # repo.remotes.origin.push()\n",
    "    # -\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2572ca2aab7f2473fba50684e7980853e48f886eb6611acebd0d880cf59cd0f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
